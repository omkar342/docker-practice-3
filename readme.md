# Clone the repository

```bash
git clone
```

# Turn on docker by opening the docker desktop


# Docker Commands for docker-practice

- **Docker Build**: Builds an image from a Dockerfile

  ```
  docker build -t docker-practice .
  ```

- **Docker Run**: To run docker file

  ```
  docker run -d -p 3002:3000 -e PORT=3000 docker-practice
  ```

**Description**: Above Command runs a Docker container in detached mode (-d) with port mapping, mapping port 4000 on the host to port 3001 in the container. The container is named docker-practice.

- **Docker Execution**: To execute a command in a running Docker container, use the `docker exec` command. For example, to list the files in the root directory of a running container, you can run:

  ```
  docker exec -it <container_name_or_id> ls

  ```

**Docker Exec**: Executes a co mmand in a running Docker container. Replace <container_name_or_id> with the actual ID of the running Docker container where you want to execute the command.

# Dockerfile Explanation

## 1. `FROM node:20`

This instruction specifies the base image to use for building this Docker image. It instructs Docker to pull the official Node.js image with the tag `20`. This image contains the Node.js runtime environment.

## 2. `WORKDIR /app`

This instruction sets the working directory inside the Docker container to `/app`. Subsequent instructions will be executed relative to this directory.

## 3. `COPY . .`

This instruction copies all files and directories from the current directory on the host (where the Docker build context is located) to the `/app` directory inside the Docker container.

## 4. `RUN npm i`

This instruction runs the `npm install` command inside the Docker container. It installs the dependencies listed in the `package.json` file of the application.

## 5. `RUN npx prisma generate`

This instruction runs the Prisma CLI command `npx prisma generate` inside the Docker container. Prisma is a modern database toolkit used for database access in Node.js applications. This command generates Prisma client code based on your Prisma schema.

## 6. `RUN npm run build`

This instruction runs the `npm run build` command inside the Docker container. It typically compiles TypeScript code, bundles assets, or performs any other build steps defined in the `scripts` section of the `package.json` file.

## 7. `CMD ["node", "dist/index.js"]`

This instruction specifies the default command to run when a container is started from this Docker image. It runs the Node.js application entry point, which is typically the `index.js` file located in the `dist` directory after the build step.


# Docker Volumes Example

This example demonstrates how to use Docker volumes to persist data in a MongoDB container.

## Without Volumes

By default, when you run a MongoDB container without specifying a volume, the data stored inside the container is lost when the container is stopped or restarted.

```bash
# Start a MongoDB container
docker run -p 27017:27017 -d mongo

# Open MongoDB Compass and connect to the running container. Add some data to the database.

# Kill the container
docker kill <container_id>

# Restart the container
docker run -p 27017:27017 -d mongo

# Open MongoDB Compass again and try to explore the database. The data you added earlier is no longer present.
```

# With Volumes

To persist data across container restarts, you can use Docker volumes. Volumes are managed by Docker and provide a way to store data outside the container's filesystem.

```bash
# Create a volume
docker volume create volume_database

# Start a MongoDB container and mount the volume to the /data/db directory (where MongoDB stores its data)
docker run -v volume_database:/data/db -p 27017:27017 -d mongo

# Open MongoDB Compass and connect to the running container. Add some data to the database.

# Kill the container
docker kill <container_id>

# Restart the container, making sure to mount the same volume
docker run -v volume_database:/data/db -p 27017:27017 -d mongo
```

# Open MongoDB Compass again and explore the database. The data you added earlier is still present.


By using volumes, you can ensure that important data generated by containers is stored separately from the container's filesystem. This allows the data to survive container restarts and enables data persistence across different container instances.

# Additional Volume Commands

Here are a few additional commands related to volumes in Docker:

```bash

# List all volumes
docker volume ls

# Inspect a specific volume
docker volume inspect <volume_name>

# Remove a volume
docker volume rm <volume_name>

# Remove all unused volumes
docker volume prune


```

Networks
--------

In Docker, networks provide a way for containers to communicate with each other and with the outside world. By default, Docker containers are isolated and cannot communicate with each other directly. However, by attaching containers to the same network, you can enable communication between them.

![notion image](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fdd624914-6876-4b58-9694-424f7aa5e22a%2Ff8d2f448-458c-4839-8b3f-1dbfceafd38b%2FUntitled.png?table=block&id=2c840075-3b94-4a95-b4fd-00046e4cb77d&cache=v2)

#### [](https://app.100xdevs.com/courses/2/234/256#f5837f1555364420874ef1a6ca3e0418 "Understanding Networks")Understanding Networks

Let's explore how to make containers talk to each other using Docker networks:

1.  Clone the repository:

`git clone <https://github.com/100xdevs-cohort-2/week-15-live-2.2.git>`

1.  Navigate to the cloned repository:

`cd week-15-live-2.2`

1.  Build the Docker image:

`docker build -t image_tag .`

Replace `image_tag` with a meaningful tag for your image (e.g., `my-app`).

1.  Create a custom network:

`docker network create my_custom_network`

This command creates a new Docker network named `my_custom_network`.

1.  Start the backend container with the network attached:

`docker run -d -p 3000:3000 --name backend --network my_custom_network image_tag`

This command starts a container from the built image, maps port 3000 from the container to port 3000 on the host, names the container `backend`, and attaches it to the `my_custom_network` network.

1.  Start the MongoDB container on the same network:

`docker run -d -v volume_database:/data/db --name mongo --network my_custom_network -p 27017:27017 mongo`

This command starts a MongoDB container, mounts the `volume_database` volume to persist data, names the container `mongo`, attaches it to the `my_custom_network` network, and maps port 27017 from the container to port 27017 on the host.

1.  Check the logs to ensure the database connection is successful:

`docker logs <container_id>`

Replace `<container_id>` with the ID of the backend container. The logs should indicate a successful connection to the MongoDB database.

1.  Visit an endpoint in your application to ensure it can communicate with the database.

Note: If you don't need to access the MongoDB container from outside the Docker network, you can remove the `-p 27017:27017` port mapping.

![notion image](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fdd624914-6876-4b58-9694-424f7aa5e22a%2Fc488cbd1-e627-450b-8c30-2a22c6340451%2FUntitled.png?table=block&id=0dadc7c8-69e2-4e16-89e5-b348484451dc&cache=v2)

#### [](https://app.100xdevs.com/courses/2/234/256#ada426557e46444ca710469160bed29c "Types of Networks")Types of Networks

Docker provides different types of networks, each serving a specific purpose:

1.  Bridge: The default network driver for containers. When you run a container without specifying a network, it's attached to a bridge network. It provides a private internal network on the host machine, and containers on the same bridge network can communicate with each other using their container names or IP addresses.

1.  Host: This network removes the network isolation between the container and the Docker host, and uses the host's networking directly. Containers attached to the host network can directly access the host's network stack, which is useful for services that need to handle lots of traffic or expose many ports.

Example of running a container with the host network:

`docker run --network host my-image`

1.  None: This network disables networking for a container, providing complete network isolation. Containers attached to the "none" network have no external connectivity.

Example of running a container with no network:

`docker run --network none my-image`

1.  Overlay: Overlay networks allow communication between containers running on different Docker hosts, enabling multi-host networking. This is useful in swarm mode for connecting services across multiple nodes.

1.  Macvlan: Macvlan networks allow containers to have their own MAC addresses and appear as physical devices on the network. This is useful for legacy applications that expect to be directly connected to the physical network.

> By leveraging Docker networks, you can enable communication between containers, control network isolation, and architect multi-container applications effectively. Networks provide flexibility and scalability in managing container communication and connectivity.

Pushing to Docker Hub
---------------------

Docker Hub is a public registry where you can store and share Docker images. It allows you to distribute your images to others and also provides a centralized repository for your own images.

![notion image](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fdd624914-6876-4b58-9694-424f7aa5e22a%2F340ae9e0-645e-4986-a452-4305d44d01e6%2FUntitled.png?table=block&id=92da5f79-a057-4ce7-bc42-f1b3c43e3ec7&cache=v2)

Here's how you can push your Docker images to Docker Hub:

1.  Sign up for a Docker Hub account:

-   Go to the Docker Hub website (<https://hub.docker.com/>) and create an account if you don't have one already.

1.  Create a new repository:

-   Once logged in, click on the "Repositories" tab and then click on "Create Repository."

-   Provide a name for your repository and set the visibility (public or private).

-   Click "Create" to create the repository.

1.  Log in to the Docker CLI:

-   Open your terminal and log in to Docker Hub using the `docker login` command.

-   Enter your Docker Hub username and password when prompted.

-   If you have enabled two-factor authentication, you might need to create an access token. Follow the instructions in the Docker documentation (<https://docs.docker.com/security/for-developers/access-tokens/>) to create an access token.

1.  Push your image to the repository:

-   Before pushing, make sure your image is tagged with the correct repository name and tag.

-   Use the `docker tag` command to tag your image:

`docker tag your_image_name your_username/your_reponame:tagname`

-   Push the tagged image to Docker Hub:

`docker push your_username/your_reponame:tagname`

#### [](https://app.100xdevs.com/courses/2/234/257#b4aaa9ea670248d3b59818c2e11dea73 "Creating a New Repository and Pushing to It")Creating a New Repository and Pushing to It

Let's say you have a custom Docker image that you want to push to a new repository on Docker Hub. Here's how you can do it:

1.  Build your Docker image:

-   Use the `docker build` command to build your image and give it a meaningful name:

`docker build -t your_username/your_reponame:tagname .`

-   Replace `your_username` with your Docker Hub username, `your_reponame` with the desired repository name, and `tagname` with a tag for your image (e.g., `v1`, `latest`).

1.  Push the image to Docker Hub:

-   After building the image, push it to Docker Hub using the `docker push` command:

`docker push your_username/your_reponame:tagname`

1.  Running the image from Docker Hub:

-   Once the image is pushed to Docker Hub, you can run it on any machine that has Docker installed:

`docker run -p 3000:3000 your_username/your_reponame:tagname`

-   This command will pull the image from Docker Hub (if not already present locally) and start a container based on that image.

#### [](https://app.100xdevs.com/courses/2/234/257#fa49069bc5b64406b6680a0325d09d3d "Image Tags and Versioning")Image Tags and Versioning

When pushing images to Docker Hub, you can use tags to version your images. Tags allow you to have multiple versions of an image within the same repository. This is similar to using tags or branches in version control systems like Git.

For example, you can tag your images with version numbers or specific tags like `v1`, `v2`, `latest`, `dev`, etc. This helps in managing different versions of your image and allows users to pull specific versions based on their requirements.

To push an image with a specific tag, you can use the `docker tag` command before pushing:

`docker tag your_image_name your_username/your_reponame:v1
docker push your_username/your_reponame:v1`

#### [](https://app.100xdevs.com/courses/2/234/257#b6d803dad52945f0be1b1a123d52a269 "Sharing and Collaboration")Sharing and Collaboration

Docker Hub makes it easy to share and collaborate on Docker images. By pushing your images to Docker Hub, you can:

-   Share your images with others, allowing them to use your pre-built images in their projects.

-   Collaborate with team members by granting them access to your private repositories.

-   Automate builds and deployments by integrating Docker Hub with CI/CD pipelines.

For example, if you have pushed an image to Docker Hub, others can easily run it on their machines using a simple `docker run` command:

`docker run -p 3000:3000 your_username/your_reponame:tagname`

This command will pull the image from Docker Hub (if not already present locally) and start a container based on that image, making it convenient for others to use your pre-built images without the need for local setup or configuration.

> By leveraging Docker Hub, you can streamline the distribution and sharing of your Docker images, making it easier for others to use and collaborate on your projects.

Docker Compose
--------------

Docker Compose is a powerful tool that allows you to define and run multi-container Docker applications using a single YAML file. It simplifies the process of configuring and managing multiple containers, networks, and volumes, making it easier to develop, test, and deploy complex applications.

Before Docker Compose, you would need to manually create networks, volumes, and start containers individually, specifying all the necessary options and configurations. This process could be tedious and error-prone, especially when dealing with multiple containers and their dependencies.

![notion image](https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fdd624914-6876-4b58-9694-424f7aa5e22a%2F370637e7-3b5f-4abc-b537-cc6e0f854491%2FUntitled.png?table=block&id=1f4407c9-624e-48aa-be12-722580317635&cache=v2)

#### [](https://app.100xdevs.com/courses/2/234/257#bef36da4efb8449dbbc096157de9c226 "Problem Solved by Docker Compose")Problem Solved by Docker Compose

Docker Compose solves the problem of managing multiple containers and their dependencies by providing a declarative way to define and run multi-container applications. Instead of manually running multiple `docker` commands to create networks, volumes, and start containers, you can define your entire application stack in a single `docker-compose.yaml` file.

The `docker-compose.yaml` file allows you to specify the services (containers), networks, volumes, and their configurations in a structured and readable format. This makes it easier to understand the application architecture, share the setup with others, and version control the configuration.

#### [](https://app.100xdevs.com/courses/2/234/257#f74be6e804844def8956bb4570bd8cc8 "Before Docker Compose")Before Docker Compose

Before Docker Compose, you would need to perform the following steps to set up a multi-container application:

1.  Create a network:

`docker network create my_custom_network`

1.  Create a volume:

`docker volume create volume_database`

1.  Start the MongoDB container:

`docker run -d -v volume_database:/data/db --name mongo --network my_custom_network mongo`

1.  Start the backend container:

`docker run -d -p 3000:3000 --name backend --network my_custom_network backend`

As you can see, this process involves running multiple `docker` commands, specifying the options for each container, and manually connecting them to the appropriate network and volumes.

#### [](https://app.100xdevs.com/courses/2/234/257#92fce28b98f5410bb8d872a7ae3532c8 "After Docker Compose")After Docker Compose

With Docker Compose, you can simplify the above process by defining your application stack in a `docker-compose.yaml` file. Here's an example:

```
version: "1.0"
services:
  mongodb: 
    image: mongo:latest
    container_name: mongodb_3
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data_3:/data/db
    
  backend:
    build: .
    container_name: backend_3
    depends_on:
      - mongodb
    ports: 
      - "3000:3000"
    environment:
      - MONGO_URL=mongodb://mongodb_3:27017/test_3

volumes:
  mongodb_data_3:
```

In this `docker-compose.yaml` file:

-   We define two services: `mongodb` and `backend`.

-   The `mongodb` service uses the `mongo` image, maps port 27017, and mounts a named volume `mongodb_data` to persist data.

-   The `backend` service uses the `backend` image, depends on the `mongodb` service, maps port 3000, and sets an environment variable `MONGO_URL` to connect to the MongoDB container.

-   We define a named volume `mongodb_data` to persist MongoDB data.

To start the application stack defined in the `docker-compose.yaml` file, you simply run:

`docker-compose up`

This command reads the `docker-compose.yaml` file, creates the necessary networks and volumes, and starts the containers defined in the services section.

To stop the application stack and remove the containers, networks, and volumes, you can run:

`docker-compose down --volumes`

The `--volumes` flag ensures that the volumes are also removed along with the containers and networks.

#### [](https://app.100xdevs.com/courses/2/234/257#0924b94b8169461e9d16fc30b9e21f7e "Benefits of Docker Compose")Benefits of Docker Compose

Using Docker Compose offers several benefits:

1.  Simplified application definition: Docker Compose allows you to define your entire application stack in a single YAML file, making it easier to understand and manage the configuration.

1.  Easy sharing and collaboration: The `docker-compose.yaml` file can be version-controlled and shared with others, enabling easy collaboration and reproducibility of the application setup.

1.  Reduced complexity: Docker Compose abstracts away the complexity of managing multiple containers, networks, and volumes, providing a higher-level abstraction for defining and running multi-container applications.

1.  Improved development workflow: Docker Compose enables developers to easily set up and tear down development environments, making it faster to iterate and test changes.

1.  Portability: The `docker-compose.yaml` file can be used across different environments (development, staging, production) and on different machines, ensuring consistency and portability of the application setup.

> By leveraging Docker Compose, you can streamline the process of defining, running, and managing multi-container Docker applications, making it easier to develop, test, and deploy complex applications.